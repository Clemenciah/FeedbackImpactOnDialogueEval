# LLMCrowdDialogueEval
This repository consist of human and LLM annotated dialogues on four dialogue aspects and prompts used in the LLM annotation from the paper:  Rethinking the Evaluation of Dialogue Systems: Effects of User Feedback on Crowdworkers and LLMs
